{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Bidirectional, Layer, GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D, Concatenate, Dropout, Add, Attention\n",
    "\n",
    "from tensorflow.keras.optimizers.legacy import Adam, SGD, RMSprop\n",
    "from tensorflow.keras.losses import BinaryCrossentropy, MeanSquaredError\n",
    "\n",
    "from tensorflow.keras.regularizers import L1L2\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Layer, Dense, Dot, Activation\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "\n",
    "from attention import Attention\n",
    "\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "# Custom callback to stop training when AUC exceeds 70%\n",
    "class AUCThresholdCallback(Callback):\n",
    "    def __init__(self, threshold=0.7):\n",
    "        super(AUCThresholdCallback, self).__init__()\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        auc = logs.get('val_auc')\n",
    "        if auc is not None and auc > self.threshold:\n",
    "            print(f\"\\nStopping training as AUC reached {auc:.2f}, which is above the threshold of {self.threshold}.\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "auc_callback = AUCThresholdCallback(threshold=0.7)\n",
    "\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='attention_weight', shape=(input_shape[-1], input_shape[-1]), initializer='random_normal', trainable=True)\n",
    "        self.b = self.add_weight(name='attention_bias', shape=(input_shape[-1],), initializer='zeros', trainable=True)\n",
    "        self.u = self.add_weight(name='attention_u', shape=(input_shape[-1],), initializer='random_normal', trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        u_it = tf.tanh(tf.tensordot(inputs, self.W, axes=1) + self.b)\n",
    "        a_it = tf.nn.softmax(tf.tensordot(u_it, self.u, axes=1), axis=1)\n",
    "        output = inputs * tf.expand_dims(a_it, -1)\n",
    "        return tf.reduce_sum(output, axis=1)\n",
    "\n",
    "num_heads = 20\n",
    "key_dim = 8\n",
    "\n",
    "class SelfAttention_head(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads, key_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.key_dim = key_dim\n",
    "        self.attention = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        attn_output = self.attention(inputs, inputs)\n",
    "        return attn_output\n",
    "    \n",
    "\n",
    "# class SelfAttention(Layer):\n",
    "#     def __init__(self, units):\n",
    "#         super(SelfAttention, self).__init__()\n",
    "#         self.units = units\n",
    "#         self.W = Dense(units)\n",
    "#         self.U = Dense(units)\n",
    "#         self.V = Dense(1)\n",
    "    \n",
    "#     def call(self, inputs):\n",
    "#         # inputs.shape = (batch_size, timesteps, hidden_size)\n",
    "#         score = tf.nn.tanh(self.W(inputs) + self.U(inputs))\n",
    "#         attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "#         context_vector = attention_weights * inputs\n",
    "#         context_vector = tf.reduce_sum(context_vector, axis=1)  # Weighted sum of the inputs\n",
    "#         return context_vector, attention_weights\n",
    "    \n",
    "\n",
    "class SelfAttention(Layer):\n",
    "    def __init__(self, units, l1=1e-5, l2=1e-4, **kwargs):\n",
    "        super(SelfAttention, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "        self.query_dense = Dense(units, kernel_regularizer=L1L2(l1=self.l1, l2=self.l2))\n",
    "        self.key_dense = Dense(units, kernel_regularizer=L1L2(l1=self.l1, l2=self.l2))\n",
    "        self.value_dense = Dense(units)  # No regularization on value, as it's just a projection\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # Query, Key, Value transformations\n",
    "        query = self.query_dense(inputs)\n",
    "        key = self.key_dense(inputs)\n",
    "        value = self.value_dense(inputs)\n",
    "        \n",
    "        # Multiplicative self-attention (dot-product)\n",
    "        attention_scores = Dot(axes=-1)([query, key])\n",
    "        attention_weights = Activation('tanh')(attention_scores)\n",
    "        \n",
    "        # Apply the attention weights to the value vectors\n",
    "        context_vector = Dot(axes=1)([attention_weights, value])\n",
    "        \n",
    "        return context_vector\n",
    "    \n",
    "# l1 = 0.0001\n",
    "# l2 = 0.0001\n",
    "\n",
    "l1 = None\n",
    "l2 = None\n",
    "\n",
    "lstm_dropout = 0.2\n",
    "recurrent_dropout = 0.0\n",
    "dense_dropout = 0.3\n",
    "loss = 'mse'\n",
    "# loss = 'binary_crossentropy'\n",
    "# loss = 'mse'\n",
    "\n",
    "n_target = Y_train.shape[-2:]\n",
    "\n",
    "regularization_criteria = L1L2(l1=l1, l2=l2)\n",
    "# regularization_criteria = None\n",
    "\n",
    "n_target_y = Y_train.shape[-1]\n",
    "\n",
    "def build_model(input_shape):\n",
    "    input_layer = Input(shape=input_shape, name='Input_1')\n",
    "    sequence = input_layer\n",
    "    sequence = LSTM(50, return_sequences = True, recurrent_activation = 'tanh', activation = 'tanh', name=\"LSTM-1\", kernel_regularizer = regularization_criteria, dropout=lstm_dropout, recurrent_dropout=recurrent_dropout)(input_layer)\n",
    "    # LSTM_1 = sequence\n",
    "    # LSTM_2 = LSTM(50, return_sequences = True, activation = 'tanh', name=\"LSTM-2\", kernel_regularizer = regularization_criteria, dropout=lstm_dropout, recurrent_dropout=recurrent_dropout)(LSTM_1)\n",
    "    \n",
    "    # sequence = Bidirectional(LSTM(50, return_sequences=True, recurrent_activation = 'tanh', activation = 'tanh', kernel_regularizer = regularization_criteria, dropout=lstm_dropout, recurrent_dropout=recurrent_dropout), name=\"BiLSTM-1\")(sequence)\n",
    "    # sequence = Bidirectional(LSTM(25, return_sequences=True, activation = 'tanh', kernel_regularizer = regularization_criteria, dropout=lstm_dropout, recurrent_dropout=recurrent_dropout), name=\"BiLSTM-2\")(sequence)\n",
    "    # LSTM_3 = LSTM(200, return_sequences = True, activation = 'tanh', name=\"LSTM-3\", kernel_regularizer = regularization_criteria, dropout=lstm_dropout, recurrent_dropout=recurrent_dropout)(BiLSTM_2)\n",
    "    # sequence = LSTM(200, return_sequences = True, activation = 'tanh', name=\"LSTM-4\", kernel_regularizer = regularization_criteria, dropout=lstm_dropout, recurrent_dropout=recurrent_dropout)(LSTM_3)\n",
    "    \n",
    "    \n",
    "    # sequence = BatchNormalization()(sequence)\n",
    "    # sequence = Add( name='Con_1')([sequence,LSTM_1])\n",
    "    # sequence = BatchNormalization()(sequence)\n",
    "\n",
    "    # sequence = Attention(units=10, score='luong')(sequence)\n",
    "\n",
    "    # sequence = Add( name='Con_1')([sequence,sequence])\n",
    "    # sequence = BatchNormalization()(sequence)\n",
    "\n",
    "    # sequence = SelfAttention(100, l1=l1, l2=l2)(sequence)\n",
    "    # sequence = SelfAttention_head(num_heads, key_dim)(sequence)\n",
    "    # # sequence = AttentionLayer()(sequence)\n",
    "    # sequence = BatchNormalization()(sequence)\n",
    "\n",
    "    sequence = Flatten()(sequence)\n",
    "\n",
    "    sequence = Dense(20, activation = 'tanh', name='Dense_1')(sequence)\n",
    "    sequence = Dropout(dense_dropout, name='Dropout_1')(sequence)\n",
    "    sequence = Dense(10, activation = 'tanh', name='Dense_2')(sequence)\n",
    "    # sequence = Dropout(dense_dropout, name='Dropout_2')(sequence)\n",
    "    # sequence = Dense(10, activation = 'tanh', name='Dense_3')(sequence)\n",
    "    # sequence = Dropout(dense_dropout, name='Dropout_3')(sequence)\n",
    "    sequence = Dense(n_target_y, activation = 'tanh', name='Dense_4')(sequence)\n",
    "    # sequence = tf.keras.layers.Activation(\"softmax\")(sequence)\n",
    "    # sequence = Lambda(lambda x: (x + 1) / 2) (sequence)\n",
    "    output_layer = sequence\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    return model\n",
    "\n",
    "# input_shape = (len(X_train), 8)\n",
    "input_shape = (look_back, X_train.shape[2])\n",
    "\n",
    "# lstm_units = 50\n",
    "# dense_units = 10\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "model = build_model(input_shape)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=METRICS)\n",
    "model.summary()\n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=5, batch_size=64, validation_data=(X_test, Y_test),\n",
    "                        # callbacks=[auc_callback],\n",
    "                        # class_weight=class_weight_dict\n",
    "                      )\n",
    "\n",
    "end_time = time.time()\n",
    "difference_in_seconds = end_time - start_time\n",
    "print(f\"The difference in seconds is: {difference_in_seconds}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
